{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get total dataset size\n",
    "This [Jupyter](http://jupyter.com) notebook was used to see how big the datasets were that we analized in the [publication on the acinar complexity](https://www.authorea.com/274247/47HwqAxume3L2xkLOsg_SQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import glob\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from \\\\nas.ana.unibe.ch\\\\gruppe_schittny\\Data\\doc\\David\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'debian' in platform.dist():\n",
    "    drive = os.path.join(os.sep, 'home', 'habi', 'nas_gruppe_schittny')\n",
    "else:\n",
    "    drive = os.path.join('\\\\\\\\nas.ana.unibe.ch\\\\', 'gruppe_schittny', 'Data')\n",
    "# Load the data from this folder\n",
    "RootPath = os.path.join(drive, 'doc', 'David')\n",
    "print('We are loading all the data from %s' % RootPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of *all* excel files that Eveline exported from the STEPanizer\n",
    "# Based on https://stackoverflow.com/a/14798263\n",
    "StepanizerFiles_Eveline = sorted(glob.glob(os.path.join(RootPath, '**/*201[1234567]*result.xls'), recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Animals = [os.path.basename(f).split('_R108C')[1].split('mrg-')[0][:3] for f in StepanizerFiles]  # all animals\n",
    "Animals = sorted(list(set(Animals)))  # unique ones: https://stackoverflow.com/a/27305828/323100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eveline counted the alveoli in 285 acini\n"
     ]
    }
   ],
   "source": [
    "print('Eveline counted the alveoli in %s acini' % len(StepanizerFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eveline assessed 13 animals\n",
      "- 04A\n",
      "- 04B\n",
      "- 04C\n",
      "- 10A\n",
      "- 10B\n",
      "- 10C\n",
      "- 21B\n",
      "- 21D\n",
      "- 21E\n",
      "- 60B\n",
      "- 60C\n",
      "- 60D\n",
      "- 60E\n"
     ]
    }
   ],
   "source": [
    "print('Eveline assessed %s animals' % len(Animals))\n",
    "for anml in sorted(Animals):\n",
    "    print('-', anml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the reconstructions on `anatera4`, where we still have all the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from \\\\anatera4\\share\\SLS\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'debian' in platform.dist():\n",
    "    drive = '/run/user/1000/gvfs/smb-share:server=anatera4,share='\n",
    "else:\n",
    "    drive = '\\\\\\\\anatera4\\\\'\n",
    "# Load the data from this folder\n",
    "terastation = drive + os.path.join('share', 'SLS')\n",
    "print('We are loading all the data from %s' % terastation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from other notebook, where we have the information on all the assessed original DICOM files\n",
    "try:\n",
    "    VolumesFromDisk = pandas.read_pickle(max(glob.iglob('VolumesFromDisk*.pkl'), key=os.path.getctime))\n",
    "except ValueError:\n",
    "    print('I was not able to find \"VolumesFromDisk.pkl\". '\n",
    "          'Please run \"Analysis.ipynb\" where this file is generated...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique sample directories in one step (see https://stackoverflow.com/a/26032781/323100)\n",
    "SampleDirectories = {os.path.dirname(i) for i in VolumesFromDisk.Location_Volume.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique beamtime folders in one step\n",
    "BeamtimeDirectories = {os.path.dirname(i) for i in SampleDirectories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* sample folders\n",
    "SampleFolders = []\n",
    "# For each of the relevant beamtimes...\n",
    "for i in BeamtimeDirectories:\n",
    "    # ...get all folders and subfolders\n",
    "    for root, directories, files in os.walk(i):\n",
    "        # Search in each found directory...\n",
    "        for directory in directories:\n",
    "            # ...if we find a folder which name matches one of the Animals\n",
    "            for animal in Animals:\n",
    "                if str('C' + animal) in directory:\n",
    "                    SampleFolders.append(os.path.join(root, directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the information into a dataframe\n",
    "DataDetails = pandas.DataFrame()\n",
    "DataDetails['Location'] = SampleFolders\n",
    "DataDetails['Sample'] = [os.path.basename(s) for s in DataDetails['Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for all the reconstructions\n",
    "DataDetails['Reconstructions'] = [glob.glob(os.path.join(l, '*rec*bit*', '*.tif')) for l in DataDetails['Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the original TIF files\n",
    "DataDetails['Sizes'] = [[os.stat(rec).st_size for rec in recs] for recs in DataDetails['Reconstructions']]\n",
    "DataDetails['TotalSize'] = [sum(sizes) for sizes in DataDetails['Sizes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, the reonstructions of each of the 42 assessed samples are 8.00 GB in size\n"
     ]
    }
   ],
   "source": [
    "print('In total, the reonstructions of each of the %s assessed samples '\n",
    "      'are %0.2f GB in size' % (len(DataDetails),\n",
    "                                1e-9 * float(DataDetails.TotalSize.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, all reconstructions are 335.96 GB in size\n"
     ]
    }
   ],
   "source": [
    "print('In total, all reconstructions are %0.2f GB in size' % (1e-9 * float(DataDetails.TotalSize.sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
